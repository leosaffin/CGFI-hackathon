{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "64a94fd4-ac5c-4f69-b995-784d61c102ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "import geopandas as gpd\n",
    "import huracanpy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from parse import parse\n",
    "from tqdm import tqdm\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1eed803-a35d-4af0-ab3d-a382d4403658",
   "metadata": {},
   "outputs": [],
   "source": [
    "gws = Path(\"/gws/pw/j07/workshop/users/ukcgfi-hackathon/\")\n",
    "\n",
    "depresys = gws / \"data/decadal/depresys/TRACKS/\"\n",
    "depresys_glob = \"DePreSys4_*_*_*\"\n",
    "depresys_extra = \"DePreSys4_{run_id}_{start_year:04d}_{ensemble_member:d}\"\n",
    "fname = \"tr_trs_pos.2day_addT63vor_addw10m_addmslp_addprecip.tcident.new\"\n",
    "\n",
    "depresys_full = str(depresys / depresys_extra / fname)\n",
    "\n",
    "variable_names = [\n",
    "    \"vorticity850hPa\",\n",
    "    \"vorticity700hPa\",\n",
    "    \"vorticity600hPa\",\n",
    "    \"vmax10m\",\n",
    "    \"mslp\",\n",
    "    \"precip\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1779ff48-f149-46ff-b625-dec654e7291b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = sorted([str(f) for f in depresys.rglob(depresys_glob + \"/\" + fname)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58543c3-451d-4b5e-8c27-a2fbfcff1afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 630/630 [11:35<00:00,  1.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 630/630 [11:28<00:00,  1.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 630/630 [11:37<00:00,  1.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 630/630 [12:17<00:00,  1.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████▏   | 387/630 [07:01<04:30,  1.11s/it]"
     ]
    }
   ],
   "source": [
    "# Merge all tracks to a single netCDF\n",
    "for lead_years in range(2, 12):\n",
    "    print(lead_years)\n",
    "    all_tracks = []\n",
    "    for filename in tqdm(all_files):\n",
    "        try:\n",
    "            tracks = huracanpy.load(filename, variable_names=variable_names, source=\"TRACK\")\n",
    "        except ValueError:\n",
    "            tracks = huracanpy.load(filename, variable_names=variable_names, source=\"TRACK\", track_calendar=\"360_day\")\n",
    "            dt = tracks.time - tracks.time[0]\n",
    "            tracks[\"time\"] = tracks.time[0].astype(\"datetime64[s]\") + dt\n",
    "\n",
    "        info = parse(depresys_full, filename).named\n",
    "\n",
    "        tracks = tracks.isel(record=np.where(tracks.time.dt.year == info[\"start_year\"] + lead_years)[0])\n",
    "    \n",
    "        all_tracks.append(tracks.assign(\n",
    "            start_year=(\"record\", [info[\"start_year\"]] * len(tracks.time)),\n",
    "            ensemble_member=(\"record\", [info[\"ensemble_member\"]] * len(tracks.time)),\n",
    "        ))\n",
    "    \n",
    "    all_tracks = huracanpy.concat_tracks(all_tracks)\n",
    "    huracanpy.save(all_tracks, f\"tracks/depresys/depresys_tcs_{lead_years:02d}-year-lead.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "ec73d57c-127e-45d0-898e-f9675bf72243",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'huracanpy.info._geography' from '/home/users/train187/miniforge3/envs/core/lib/python3.13/site-packages/huracanpy/info/_geography.py'>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "japan = pd.read_csv(\"~/john/Areas_JKH/Japan_1.0.csv\", delimiter=\" \", header=None, names=[\"lon\", \"lat\"])\n",
    "usa = pd.read_csv(\"~/john/Areas_JKH/USA_all.csv\", delimiter=\" \", header=None, names=[\"lon\", \"lat\"])\n",
    "\n",
    "usa_gulf_mexico = pd.read_csv(\"~/john/Areas_JKH/USA1_gulf_mexico.csv\", delimiter=\" \", header=None, names=[\"lon\", \"lat\"])\n",
    "usa_florida = pd.read_csv(\"~/john/Areas_JKH/USA2_florida_1.1.csv\", delimiter=\" \", header=None, names=[\"lon\", \"lat\"])\n",
    "usa_north = pd.read_csv(\"~/john/Areas_JKH/USA3_North.csv\", delimiter=\" \", header=None, names=[\"lon\", \"lat\"])\n",
    "\n",
    "B = dict(\n",
    "    Japan=Polygon([(lon, lat) for lon, lat in zip(japan.lon, japan.lat)]),\n",
    "    USA=Polygon([(lon, lat) for lon, lat in zip(usa.lon, usa.lat)]),\n",
    ")\n",
    "huracanpy.basins[\"CGFI\"] = gpd.GeoDataFrame(index=B.keys(), geometry=list(B.values()))\n",
    "\n",
    "B = dict(\n",
    "    usa_gulf_mexico=Polygon([(lon, lat) for lon, lat in zip(usa_gulf_mexico.lon, usa_gulf_mexico.lat)]),\n",
    "    usa_florida=Polygon([(lon, lat) for lon, lat in zip(usa_florida.lon, usa_florida.lat)]),\n",
    "    usa_north=Polygon([(lon, lat) for lon, lat in zip(usa_north.lon, usa_north.lat)]),\n",
    ")\n",
    "huracanpy.basins[\"cgfi_gate\"] = gpd.GeoDataFrame(index=B.keys(), geometry=list(B.values()))\n",
    "\n",
    "from huracanpy.info import _geography\n",
    "from importlib import reload\n",
    "\n",
    "reload(_geography)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "f40de714-b2ea-4fac-847c-1448fbf064a0",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Dataset' object has no attribute 'wind'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[104]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      8\u001b[39m tracks = huracanpy.load(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mtracks/depresys/depresys_tcs_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m02d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m-year-lead.nc\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m tracks = tracks.hrcn.add_basin(convention=convention)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[43mtracks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwind\u001b[49m[tracks.wind == \u001b[32m1e25\u001b[39m] = \u001b[32m0.\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Only use time at max intensity for counts\u001b[39;00m\n\u001b[32m     13\u001b[39m tracks_apex = tracks.hrcn.get_apex_vals(\u001b[33m\"\u001b[39m\u001b[33mvmax10m\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/core/lib/python3.13/site-packages/xarray/core/common.py:306\u001b[39m, in \u001b[36mAttrAccessMixin.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m    304\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m suppress(\u001b[38;5;167;01mKeyError\u001b[39;00m):\n\u001b[32m    305\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m source[name]\n\u001b[32m--> \u001b[39m\u001b[32m306\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[32m    307\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m object has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    308\u001b[39m )\n",
      "\u001b[31mAttributeError\u001b[39m: 'Dataset' object has no attribute 'wind'"
     ]
    }
   ],
   "source": [
    "data_out = []\n",
    "\n",
    "convention = \"cgfi_gate\"\n",
    "basins = [\"usa_gulf_mexico\", \"usa_florida\", \"usa_north\"]\n",
    "\n",
    "# Loop over all forecasts\n",
    "for n in range(1, 10+1):\n",
    "    tracks = huracanpy.load(f\"tracks/depresys/depresys_tcs_{n:02d}-year-lead.nc\")\n",
    "    tracks = tracks.hrcn.add_basin(convention=convention)\n",
    "    tracks.wind[tracks.wind == 1e25] = 0.\n",
    "\n",
    "    # Only use time at max intensity for counts\n",
    "    tracks_apex = tracks.hrcn.get_apex_vals(\"vmax10m\")\n",
    "\n",
    "    # Only TC season\n",
    "    tracks = tracks.hrcn.sel_id(\n",
    "        tracks.track_id[(tracks.time.dt.month >= 6) & (tracks.time.dt.month <= 11)]\n",
    "    )\n",
    "\n",
    "    # Get cyclone counts by basin\n",
    "    for basin in basins:\n",
    "        ids_basin = tracks_apex.track_id[tracks_apex.basin == basin]\n",
    "        apex_basin = tracks.hrcn.sel_id(ids_basin)\n",
    "        tcs_basin = tracks.hrcn.sel_id(ids_basin)\n",
    "\n",
    "        mask = (tcs_basin.basin == basin).astype(int)\n",
    "        \n",
    "        wind = tcs_basin.vmax10m * mask\n",
    "        mslp = tcs_basin.mslp * mask\n",
    "        \n",
    "        ace = huracanpy.tc.ace(wind, threshold=0, sum_by=tracks.track_id)\n",
    "        pace, _ = huracanpy.tc.pace(mslp, model=\"z2021\")\n",
    "        ssi = (wind ** 3).groupby(\"track_id\").sum()\n",
    "        counts = apex_basin.basin.groupby(tcs_basin.time.dt.year).count()\n",
    "\n",
    "        data_out.append(\n",
    "            pd.DataFrame(data=dict(\n",
    "                start_year=[info[\"start_year\"]] * len(counts.year),\n",
    "                ensemble_member=[info[\"ensemble_member\"]] * len(counts.year),\n",
    "                year=counts.year.values,\n",
    "                basin=[basin] * len(counts.year),\n",
    "                count=counts.values,\n",
    "                ace=ace.groupby(tcs_basin.time.dt.year).sum().values,\n",
    "                pace=pace.groupby(tcs_basin.time.dt.year).sum().values,\n",
    "                ssi=ssi.groupby(tcs_basin.time.dt.year).sum().values,\n",
    "            ))\n",
    "        )\n",
    "\n",
    "        data_out.to_csv(f\"depresys_tc_activity_metrics_lead_time_{n}_basin_{basin}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "8810c8c9-4b0d-4204-8727-1a02322e8e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [03:39<00:00, 21.95s/it]\n"
     ]
    }
   ],
   "source": [
    "convention = \"cgfi_gate\"\n",
    "basins = [\"usa_gulf_mexico\", \"usa_florida\", \"usa_north\"]\n",
    "\n",
    "# Loop over all forecasts\n",
    "for n in tqdm(range(1, 10+1)):\n",
    "    tracks = huracanpy.load(f\"tracks/depresys/depresys_tcs_{n:02d}-year-lead.nc\")\n",
    "    tracks = tracks.hrcn.add_basin(convention=convention)\n",
    "    tracks.vmax10m[tracks.vmax10m == 1e25] = 0.\n",
    "\n",
    "    # Only use time at max intensity for counts\n",
    "    tracks_apex = tracks.hrcn.get_apex_vals(\"vmax10m\")\n",
    "    \n",
    "    # Only TC season\n",
    "    tracks = tracks.hrcn.sel_id(\n",
    "        tracks.track_id[(tracks.time.dt.month >= 6) & (tracks.time.dt.month <= 11)]\n",
    "    )\n",
    "\n",
    "    # Get cyclone counts by basin\n",
    "    for basin in basins:\n",
    "        ids_basin = tracks_apex.track_id[tracks_apex.basin == basin]\n",
    "        apex_basin = tracks_apex.hrcn.sel_id(ids_basin)\n",
    "        tcs_basin = tracks.hrcn.sel_id(ids_basin)\n",
    "        \n",
    "        mask = (tcs_basin.basin == basin).astype(int)\n",
    "        \n",
    "        wind = tcs_basin.vmax10m * mask\n",
    "        mslp = tcs_basin.mslp * mask\n",
    "        \n",
    "        ace = huracanpy.tc.ace(wind, threshold=0, sum_by=tcs_basin.track_id)\n",
    "        pace, _ = huracanpy.tc.pace(mslp, model=\"z2021\", sum_by=tcs_basin.track_id)\n",
    "        ssi = (wind ** 3).groupby(tcs_basin.track_id).sum()\n",
    "        precip = (tcs_basin.precip * mask).groupby(tcs_basin.track_id).sum()\n",
    "\n",
    "        results = xr.merge([dict(ace=ace, pace=pace, ssi=ssi, precip=precip)])\n",
    "        results = xr.merge([results, apex_basin[[\"start_year\", \"ensemble_member\", \"time\"]]])\n",
    "        results = results.sortby([\"start_year\", \"ensemble_member\", \"time\"])\n",
    "\n",
    "        results_sum = results.groupby(results.start_year.astype(str) + results.ensemble_member.astype(str)).sum()\n",
    "        results_mean = results.groupby(results.start_year.astype(str) + results.ensemble_member.astype(str)).mean()\n",
    "        results_count = results.groupby(results.start_year.astype(str) + results.ensemble_member.astype(str)).count()\n",
    "\n",
    "        results_all = xr.merge([\n",
    "            results_sum[[\"ace\", \"pace\", \"ssi\", \"precip\"]],\n",
    "            results_mean[\"start_year\"].astype(int),\n",
    "            results_mean[\"ensemble_member\"].astype(int),\n",
    "            results_count.ace.rename(\"count\"),\n",
    "        ])\n",
    "\n",
    "        results_csv = results_all.drop_vars(\"group\").to_pandas()\n",
    "\n",
    "        results_csv_all = []\n",
    "        for start_year in range(1960, 2022+1):\n",
    "            for ensemble_member in range(1, 10):\n",
    "                row = results_csv[\n",
    "                    (results_csv.start_year == start_year) &\n",
    "                    (results_csv.ensemble_member == ensemble_member)\n",
    "                ]\n",
    "        \n",
    "                if len(row) == 1:\n",
    "                    results_csv_all.append(row)\n",
    "                else:\n",
    "                    results_csv_all.append(pd.DataFrame(\n",
    "                        dict(ace=[0], pace=[0], ssi=[0], precip=[0], start_year=[start_year], ensemble_member=[ensemble_member], count=[0])\n",
    "                    ))\n",
    "\n",
    "        pd.concat(results_csv_all).to_csv(f\"depresys_tc_activity_metrics_lead_time_{n}_basin_{basin}.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cgfi",
   "language": "python",
   "name": "cgfi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
